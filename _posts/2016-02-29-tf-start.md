---
layout: post
title:  Getting my feet wet with Tensorflow
date:   2016-02-29 09:00:54 -0800
categories: machine learning
---

##### I recommend checking out my more recent content, such as this post [here](/machine/learning/2016/09/10/loss-and-small-models.html)

## Starting out

I’ve been fascinated by neural networks ever since I learned about them a few years ago. It’s been almost a magical sort of fascination. I’ve learned and mostly understood the underlying algorithms behind artificial neural nets and how they’re trained. But even after learning about them and implementing a basic one I still believe they are just magic. The idea that they can learn to seemingly anything by example is really impressive. Of course training them to learn is difficult and often limited by the lack of good data and not having adequate computational power.

Neural nets have become pretty popular recently. Wide scale usage of deep convolutional neural nets in the field of computer vision has brought some outstanding achievements in image recognition. And to further my point of neural nets being magic, people have reversed these deep neural nets to create psychadelic [‘deep-dream’](https://github.com/google/deepdream/blob/master/dream.ipynb) pictures.

A couple months ago Google released their [Tensorflow](https://www.tensorflow.org/) machine learning library. I figured that would be a good time to pursue some deep learning into deep learning myself. I decided that a fun starting project would be colorizing grayscale images. My reasoning was that it would be super simple to create a large dataset. I can simply just collect tons of images of just about anything and then convert them to grayscale using this convenient formula based on how our eyes perceive each color: ```gray = 0.29 * R + 0.60 * G + 0.11 * B```. I could even use still frames from movies as a source of data, potentially making it possible to colorize your own photos as if they were in a given movie. There are also tons of historical photos that could be recolorized. So with many applications and lots of excitement I began.

At first I didn’t put much time into this project. It wasn’t until I decided to incorporate it with my actual work (using Jupyter vs. CoLab) and saw the [popular post](http://tinyclouds.org/colorize/) on [HN](http://news.ycombinator.com). That person did pretty much exactly what I set out to do, however they used a pre-trained Imagenet model to extract features (using an interesting technique called hyper columns, definitely [check it out](http://tinyclouds.org/colorize/)). My plan was to take a bit more of a brute-force approach and train my network from scratch. Maybe that’s not the best approach, I guess we’ll find out…

## Getting up and running with Jupyter

##### **I put together a notebook where you can recreate some of the results I get. Link is [here](https://drive.google.com/file/d/0B_CaIm7pjWpfYW1lRG56Qy12NTg/view?usp=sharing). Open it in Jupyter, Colab (if at Google), or your favorite ipynb editor.**

Getting Tensorflow up and running wasn’t too difficult. It took a little bit of fiddling to get cuDNN working properly, but I really wanted to make use of my GTX970 video card. Jupyter worked straight out of the box with everything.

I started working on helper functions. I decided to create a bunch of conversions to and from numpy arrays and PIL/Image, splitting between gray/color images and RGB/LAB. Nothing too difficult here. Just tedious work of making sure shapes are correct and types are right. Ran into some issues with converting to/from RGB/LAB because the RGB values were between 0-255 and PIL Images used 0.0-1.0 when converting from RGB to LAB, dividing them by 255 did the trick. I’ll get into more detail on why I picked the LAB color space in a bit.

## The dataset

I found a dataset of many stock photos, so I got it and wrote and ran a program over the RAR files they were contained in to shrink all the images to be 150x100. This didn’t preserve aspect ratio as some images were vertical instead of horizontal or wider. Maybe I should pad all images with black, then I can preserve aspect ratio and fill in the rest with black, or just enlarge the picture and trim it. I Packaged all of the images up into an Nx150x100x3 numpy matrix, where N is the number of pictures in each RAR. Then I split that into two arrays, one with lightness (L), the other two with color (AB). Put them in a Dataset with L being the input, and AB being the output. For about 1400 images this was ~500 MB. Saved that to disk for easy reuse without processing the original RAR file.

As you can probably imagine, by just grabbing a set of stock images without really combing out the outliers, there are lots of ‘bad’ images. Some of the photos are CG generated, not supposed to represent anything tangible. For example, there’s one that’s a giant arrow made out of a maze. Other images seem to be grayscale already, and others have very artistic color schemes, which might not be consistent. Still, I figure there were enough good pictures in the set to counteract the bad. I’ll probably need to get a new dataset in the future (possibly from [ImageNet](http://image-net.org/)) if this set doesn’t work out.

## The network

I spent some time getting Tensorflow working end to end. Small sample problem worked well out of the box in Jupyter, but I had difficulty with getting the dimensions of the convolution layers and fully connected layer to match up. Nothing too bad, just a bit tedious. It would be nice to have some easy planner for architecture. I also had some problems with memory. My GTX970 card only has 4GB of memory ([or more likely 3.5GB](https://www.google.com/webhp?ie=utf-8&oe=utf-8#q=gtx+970+memory+issue)). It was very difficult to track which layers were using how much memory. When I went over it seemed to tell me which tensor it was having trouble allocing, but even that was vague and didn’t help too much. It would be nice to have some sort of memory profiler to see exactly how much memory is being used and how I can make the most of what I have. Settled with a couple convolutional layers with ~30 or so features, no pooling, followed by a fully connected layer that output [:, 150, 100, 2] where : is the batch size.

For my loss function I just used the euclidean distance (L2 norm) between the two pictures. This means sum up the squared differences between each corresponding pixel. LAB space is supposed to approximate [human vision](https://en.wikipedia.org/wiki/Lab_color_space#Advantages) and the euclidean distance between two colors is supposed to be proportional to how differently people perceive those colors (https://www.lri.fr/~gcharpia/colorization_chapter.pdf, 1.3.1).

## How to (not) train your network

I trained the network in small batches from the stock image set. I only used a subset of the total, a set on architecture. Feeding in the gray images (shaped [:,150,100,1], where : is the batch size) as input, and having the color components being the target (shaped [:,150,100,2]).

Training didn’t really work at this point. I ran it for thousands iterations and failed to see any improvement during training. I printed out the loss frequently during training, usually dividing it by a thousand or so because it was a pretty big number (the distance between two images). It seemed to jump around a bit, but it never went down significantly. This got me worried, I figured maybe outputting an image from a neural net was more difficult than I originally anticipated. Or maybe I needed more data or better data. Or maybe my network architecture wasn’t good for the problem I was trying to solve…

## Taking two steps back

There were tons of things that could have been wrong with my approach, so I decided to step back and approach a bit more of a bite-sized problem. A much simpler problem is to make an image autoencoder. An autoencoder simply recreates its input. So you put in an image, the autoencoder should output that same image. The problem should be much simpler than colorizing grayscale images, and it’s definitely a prerequisite problem. If I can’t make a neural net output the same image that it took as input, I definitely can’t make it output a colorized version of the input!

I decided to take another step even further back. Instead of using the dataset of images I had created for colorization, I decided to create my own dataset. Using numpy arrays I created thousands of images of white circles on black backgrounds. I was still using the LAB colorspace, so my images were (150,100,1). After generating a large number of these images and splitting them into a training set and a validation set, I set out again to train my network.

![polkadots](/assets/tf_start/lotsacircles.png)

One point of worry for me is that the circles tend to be cut off by the edge of the image. I’m worried that because my network uses the ‘SAME’ padding for its convolution layers, it will not necessarily see or learn ‘circles’ because its actually seeing a different shape. Less thinking - more training, though. Maybe I’ll make that my motto.

## More training

This time I decided to go with an all convolutional layer net. I settled on 6 layers, with the top layer having about 32 features, then the next two with 16, two more with 8, and one final layer with 1 feature. Each layer had a kernel size of 7x7 except for the last layer which was 4x4. That last layer is the output, so that 1 feature is the grayscale color. I used ‘SAME’ padding and a stride size of 1, so as described [here](https://www.tensorflow.org/versions/master/api_docs/python/nn.html#convolution) each layer had the same width and height as the layer above it, making it easy to take the output of the last layer as the final image. I more or less arbitrarily chose that architecture. It was likely overkill for the autoencoding problem :D.

I set out and trained this new network on the new dataset of black and white circles. If the network couldn’t learn this then I needed to take another step back, do some more research, or ask other people to look over my code and see if they could help me debug the situation. Unfortunately training followed a similar pattern to my previous attempt where loss jumped around a bit but ultimately remained motionless. I tried messing around with various parameters including the training algorithm (I tried both the Adam optimizer as well as stochastic gradient descent with various learning rates). But those didn’t seem to make much difference. I even tried messing around with the network architecture, but that was more or less hopeless.

## A fortunate turn of events

As a last ditch effort I started messing with my loss function which was originally just the euclidean distance between two pictures, taking the squared distance between each corresponding pixel value. I tried dividing it by 150x100x100 and (150x100x100)^2 because that was the furthest two grayscale pictures could be (if one picture was white, 100, and the other picture was black, 0). But even that didn’t work. Just for the hell of it I tried inverting the loss function so it was 1/distance. In that case the loss should get smaller as the distance gets larger, which is the exact opposite of what we would want. However, I tried training with it and lo and behold, my network actually appeared to be training! Loss was going down pretty regularly with iterations of training. This was great! Finally something was training. After training for a while I wanted to see exactly what my network was learning to produce. So I went ahead and threw one of the images from the validation set through it. To my complete surprise it produced a crudely drawn circle where the circle was supposed to be. It looked almost as if a toddler was given a coloring book of a circle and had trouble filling it in within the lines. I had finally taught my computer to do something! (although what I taught it was honestly pretty trivial…)

![If my kid drew this I would tape it to the fridge](/assets/tf_start/toddler_circles.png)

It’s amazing how desperately unreasonable and completely dubious changes to code can end up working out so well. I noticed though that the images were getting colored beyond the range of the implementation of LAB I’m using (The lightness range seems to be 0-100, though don’t quote me on that). Squishing that result into that range give us slightly nicer looking circles.

![Looks like my network likes to step out of bounds](/assets/tf_start/toddler_circles_normalized.png)

Now it looks like the toddler’s big sibling drew these pictures. I’ve played around a bit with these circles and found some interesting things. In fact, these circles were generated from a network that was trained in less than half an hour. Training for longer gives even better circles! I’ll leave all that for another post though.

## Next steps

* Figuring out what was wrong with my loss, and what loss function I should use.
* Look into the convolutions/weights to see what the network is learning. I believe the network could honestly just settle on passing through the pixel values to the output. However I am training with dropout, so hopefully that minimizes the risk of that.
* Train on more complex images, see if the network can generalize. ’Squeeze’ the network so it is required to pull out features instead of just pass values through.
* Experiment with different architectures to look at training rate (how quickly it can process each network feed forward) and learning rate (how quickly the loss drops).
* Publish an ipynb file so people can download and repeat my experiments easily.

## Related works/papers
* [Notebook to go along with this post](https://drive.google.com/file/d/0B_CaIm7pjWpfYW1lRG56Qy12NTg/view?usp=sharing)
* http://www.cs.toronto.edu/~tijmen/tijmen_thesis.pdf
  * Good work on an autoencoder, contains some good NN/training resources.
* http://www.jcomputers.us/vol5/jcp0507-14.pdf
* http://academypublisher.com/jmm/vol04/no04/jmm0404240247.pdf
  * Both on film colorization using NN.
* http://cs229.stanford.edu/proj2013/KabirzadehSousaBlaes-AutomaticColorizationOfGrayscaleImages.pdf
  * Uses a handful of other ML techniques to colorize images (k-means, PCA, SVMs)
* http://tinyclouds.org/colorize/
  * They had similar goals to me, however they used a pretrained imagenet network and used a technique called hypercolumns to feed the activations of the imagenet network into their own network that would colorize the image. Very interesting approach but I kind of want to start from scratch instead of using a preexisting model. This post gained much popularity, so I think I picked a good problem to work on, unfortunately I wasn’t quick enough to get my work done before this person published their stuff.
* Another paper or two that I need to look through my phone and find that I read on an airplane ride.





