---
layout: post
title:  Winning a Loss-ing Battle (an overdue update)
date:   2016-09-10 09:00:54 -0800
categories: machine learning
---

## Recap

It’s been a while since my last post. I’ve found that I enjoy messing around with code much more than writing up posts. Plus it can be difficult to get the motivation to write when I can count the likely number of people who read my posts on one hand! But it’s been long enough and I have a lot to share so I really need to post something about my progress. I’ve had some fun looking back at my first posts, seeing just how far I have come and how much I’ve learned - I’m excited to look back on this post in a few months to see how much further I’ve gotten. Also, I would love feedback on everything here - my writing, my notebooks, my ideas - feel free to message me at ti@vt.edu.

In this post I’m going to start by briefly talking about my previous post and a couple mistakes that I made in them. Then I’ll talk about what autoencoders are, so if you’re not too familiar with them then my description might help you understand what they do. I’ll go into some experiments that I ran training very deep autoencoders layer-by-layer, inspired Deep Belief Networks. Finally I’ll talk about my latest work with variational autoencoders, and trying to generate class clusters with them. I put together a [notebook](https://drive.google.com/file/d/0B_CaIm7pjWpfZ3hlek9rOWZlZTQ/view?usp=sharing) demoing results from the variational autoencoder, feel free to check it out, mess around with it, and see if you can improve it. And then I’ll end this post with what I want to explore next.


## Humble Beginnings

In my [first post](/machine/learning/2016/02/29/tf-start.html) I talked about my desire to learn Tensorflow and create a grayscale image colorizer using neural networks. I quickly decided to start with an easier problem - making an autoencoder - after I realized it might be too difficult to make a colorizer given I was a machine learning/deep learning newbie.

I quickly started creating models and training them on homespun grayscale polkadot images. I found that the inverse of my euclidean distance loss function worked. As counterintuitive as that was, it worked. I made some gifs of the models training, including at various layers in the network.

![Old polkadot image](/assets/tf_3/old_dataset.png)

All of this work was a lot of fun, but I had also made many mistakes. As I mentioned, I was previously using the inverse of the L2 Distance between the two images as my loss function. That’s counterintuitive because normally you want to reduce the distance, not increase it! But using the inverse kinda worked if I normalized the output images. I believe it works because the output pixel values exploded, but they exploded in such a way that after dividing each pixel by the value of the greatest pixel it actually resembled the input image. I later found that by simply normalizing the L2 Distance so it was much smaller, the network trained correctly. The way I did this was by replacing a reduce_sum with a reduce_mean operation in the loss. So that’s one mystery down, but I still have many more to go. Figuring out this problem was the first time I realized just how finicky loss functions can be.

Another mistake that I’m amazed I didn’t catch for a while was the positioning of my dropout layer. For some reason I assumed that having a dropout layer at the end of the network would magically apply dropout to the previous layers. I was very wrong though, and instead all it did was drop out random pixels from the output image during training. Dropout is nice because it keeps your network from relying too heavily on specific neurons. It’s like training tons of smaller subsets of your networks and then combining them all when they’re ready to be used. However, it doesn’t work well at all when you’re basically just applying it to the input of your loss function! As a result I believe most of the images turned out a bit dimmer or spottier than they should have been.

## Moving Onwards

I largely continued my previous work with autoencoders. If you’re not familiar, autoencoders are a type of neural network where you feed in an image and it tries to recreate that image using an intermediate encoding to store information about the image. One way to think about this is to imagine you are trying to tell your friend, who is a painter, how to recreate a piece of art you’re looking at using only a single [tweet](https://twitter.com/), or 140 characters. You and your friend are in different rooms so they can’t see the original piece of art, and you can’t communicate anything to them beyond just the tweet. After your friend paints what they think the original painting looks like, you get judged on how similar the paintings are, and then you repeat the process with another painting. After creating a painting and comparing it to the original, you and your friend are given a bit of time discuss strategy in person.

So you need to find a way to describe all the features in the painting - how many people are in it, what’s going on in the background, what colors are used - in just 140 characters of text. Initially it might be tough for you and your artist friend to recreate a painting that looks anything like the original. But if you work out some sort of code with your friend, you might be able to do a much better job. For example, instead of saying “There are three people in the painting,” which takes up almost a quarter of the tweet, you could agree to just start each tweet with something like “P3,” which would mean three people, or “P4,” which would mean four people. In this example, you are the encoder network, the tweet is the encoded painting, and your friend is the decoder network.

Autoencoders are a form of unsupervised learning. That means you don’t use labeled information. So instead of images with corresponding labels ‘cat,’ and ‘dog,’ you just have a bunch of images. I really like how autoencoders don’t need labels. Unlabeled images are really easy to get - simply use Google Images to get a bunch of pictures, or split a movie into a bunch of individual frames. Labeled data is much harder to come by.

For my previous posts I decided to cook up my own dataset of black and white polka dots. As you could imagine I quickly became bored working with dots, so I decided to use a dataset that’s a bit more exciting (haha) yet still very simple. I settled on the MNIST dataset which is used frequently in machine learning demos and benchmarks. The dataset consists of tens of thousands of handwritten digits in grayscale. Each image is 28x28 grayscale pixels, so the dataset is very compact. As a bonus, the data is labeled, so each digit image has a corresponding label of 0-9, though I often discarded the labels and used just the images for unsupervised training. I found this a much more enjoyable dataset to work with, especially because networks tended to train super quickly given the small size of the images. Here’s a few sample images:

![7](/assets/tf_3/mnist1.png) | ![2](/assets/tf_3/mnist2.png) | ![1](/assets/tf_3/mnist3.png) | ![0](/assets/tf_3/mnist4.png) | ![4](/assets/tf_3/mnist5.png)

## Going Too Deep, Layer by Layer

I mix a bit of reading in with my Jupyter/Tensorflow work. I enjoy reading through blog posts about what other people have been working on. Although it can sometimes be frustrating, I also work my way through academic papers. Thankfully papers in the fields of ML/stat/CV tend to be pretty short, often under 10 pages. Though I would argue they often make up for being so terse by being very information/idea dense. Not having the strongest background in math/statistics doesn’t really help, especially when it comes to notation. Coming from a background of coding where we pride ourselves in writing readable code, I’m inclined to believe mathematicians take classes in obfuscation. In code we can name our variables almost whatever we want, in math they use and reuse the same 26 characters, plus a few Greek ones, for everything. Don’t get me wrong, you can express some complex and beautiful things in a very small amount of space, but that comes at the cost of readability and ambiguity. But I digress. Papers can actually be fun to work through, and understanding them can be very rewarding. Video lectures can also be great to watch, but they quickly brings me back to my days of falling asleep during college lectures.

During my research I found a lot of talk about Restricted Boltzmann Machines, or RBMs. They are a stochastic model, not too different from your basic neural network. RBMs showed a lot of promise and helped generate interest in the field of deep learning. RBMs can be stacked to make Deep Belief Networks. That’s done by training a single layer RBM autoencoder, and then taking the output of that layer as a new training set and using that to train another single layer autoencoder. So you can in essence stack many of these autoencoders, each one slightly smaller than the one above it and trained individually, to create a much larger autoencoder. You could then use the encoded output of that large autoencoder as an input to another network, such as a classifier. Then you can train the full autoencoder/classifier stack together to create a very powerful model. Being able to do this is fantastic because you are able to create super-deep networks, something that wasn’t feasible before because of the exploding/vanising gradient problem, which makes back propagation ineffective.

Although I have also learned about many other techniques - such as batch normalization, smarter weight initialization, and using residual layer blocks - to train very deep networks (which I actually make use of later), though I really liked the idea of training a network layer-by-layer. I also liked the idea of dimensionality reduction, and being able to visualize MNIST in a two dimensional space. Take a look at [this post](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) to see some interesting ways we can visualize the MNIST dataset in two dimensions.

So I decided to combine the two ideas, and create a very deep autoencoder network, trained layer-by-layer, that encodes the MNIST dataset into just the output of two neurons. So you input an image into the network, the input goes through each layer, its representation getting smaller and more compact with each step, until it’s squeezed into just two numbers.

While training these networks I began to realize how cumbersome Tensorflow is if you try to define every network you want to create explicitly. Especially since I was trying to do things a bit strangely with my layer-by-layer training. So while my first few attempts involved a lot of hasty copying and pasting of layer code, then debugging for hours and hours only to realize I forgot to change one value in one of my pasted layers, I gradually started building my own little layer library. This made it possible to define a network 15 layers deep without pulling my hair out! It even allowed me to do some interesting things, such as sharing weights between the encoder and decoder.

One thing that I found difficult to do using Tensorflow was to re-use parts of networks. Say for example I create an end-to-end autoencoder. You input an image and get an image back out. Now suppose I want to use that same network that I just trained, but instead of inputing images, I want to input encoded points, and get a novel image as an output. Then I could use the network as a generative model. In order to do that though I need to re-use the weights from the network I trained and reconnect them into a new graph that ends with the input being the encoded points. I find it kind of annoying that I need to recreate the network rather than just being able to ‘chop’ the network in half and ‘inject’ a new input to the network. Maybe there’s an easy way to do this, but I basically just ended up having to implement this in my small layer class. When I started using [TFLearn](https://github.com/tflearn/tflearn) (a higher level library over Tensorflow) the problem got even worse because the weights were hidden, so I couldn’t reuse them without hacking the variable scope/naming to allow me to. However once they updated the library it became too cumbersome for me to maintain my fork with those changes, so I gave up. Although it’s super hacky, I’m now using a technique where I pass a 1 or 0 into the network when I evaluate that basically tells the network whether it should take points from the encoder, or if it should take points directly from the input. It then adds them together after multiplying them by the binary switch, so we only get one or the other. As hacky as it is, it works. This problem has honestly been one of my greatest pain points using Tensorflow (and TFLearn), and if anyone knows an elegant solution to it, I would love to hear.

I also decided to mess around with another idea, for no good reason. One thing I did was I used the same weights for the encoder and decoder. Because corresponding layers of the encoder and decoder are pretty much just inverse functions, the weights we use take the same shape. Unfortunately I never got around to experimenting with this thoroughly. So I can’t say for sure if it’s worth it. On one hand you use up half the memory to store the weights, but you also need to store much more information in them. Maybe I’ll spend more time looking into this in the future.

In the end my results honestly weren’t too exciting. The network usually did only a mediocre job of recreating the input after finally making it down to the 2-dimension layer. So I decided to work on a different sort of problem.

## Mixing It Up

I was reading a bit about [variational autoencoders](https://arxiv.org/abs/1312.6114) (VAE). They are similar to normal autoencoders in the sense that you plug in an image and get an image that (hopefully) looks the same out. However, they are special because when training, you also try to push the encoded points into a specific kind of distribution. Instead of outputting points, the encoder outputs a distribution that you can then sample from to get points for the decoder. A typical distribution that people use is the standard normal distribution (think of a bell curve with its center at zero and a variance of one). When we use the normal distribution we can think of each encoded dimension as being a little dial that tunes the output of the decoder. Because of this, variational autoencoders tend to be good generative models. This can be seen in [this](http://vdumoulin.github.io/morphing_faces/online_demo.html) cool demo I found online. Notice how each toggle has a noticeable effect on different features of the faces.

So I started working on making a variational autoencoder. At this point I decided to use TFLearn for getting the training set and for my layers. TFLearn is convenient because it has implementations of many layer types, including batch normalization layers. I actually even contributed a little to TFLearn by submitting a small fix for their convolutional transpose layer. However, I decided to just use a couple fully connected layers for my variational autoencoder. I experimented a little with using deeper/convolutional architectures but didn’t see a whole lot of improvement.

At one point my variational autoencoder wasn’t training very well. What happened was it basically just treated the output of the encoder as random noise. That meant that it wouldn’t encode any information about how to reconstruct the input. So the decoder ended up just producing what was almost like an average of the entire MNIST dataset and the encoder just produced a wonderful standard normal distribution. 

![Average of the MNIST dataset](/assets/tf_3/average_mnist.png)

I could not for the life of me figure out what was going wrong with my network. I tried adjusting the number/width of each layer. I messed with how I initialized weights. I tried normalization at various points. I tried adjusting the number of values used for encoding. Eventually I started just screwing around with my loss function, almost in a similar fashion with my previous post and how I spontaneously decided to invert my loss function. However this time I just decided to linearly scale my loss function, so I multiplied the image loss (L2-distance) part of it by 10. Turns out that made it actually work! Instead of just producing noise it actually started producing images that resembled the input. I continued to mess around with it and changed other factors, like performing a reduce_sum instead of the reduce_mean in the loss function (which is just about equivalent to scaling by a linear factor). I’ve decided that if something isn’t training the way I want it to, that messing with the loss functions can often produce some good results. I feel like especially if you are optimizing over multiple loss functions it’s very likely for those functions to change at vastly different scales, and that the optimizer might put a lot more ‘effort’ into reducing the low hanging fruit. So by scaling the functions by either multiplying them by a linear factor, squaring it, square rooting it, exponentiating it, taking the log, etc., we can try to match their scales. I’m not sure how solid this advice is, you might be able to gain much more insight into this by diving into the math, but it’s a good way to poke around in the dark. I’m really curious to see what might happen if I adjust the scales of the loss functions throughout training. Maybe I’ll experiment with that in the future.

My plots initially came out as a huge jumbled mess. I couldn’t see anything that resembled a cluster. It looked like each class was just normally distributed around the center. Turns out I was just plotting my points incorrectly. Unfortunately it took me a while to figure out this was a problem, I just thought I couldn’t see any shape/cluster the network had created. When I finally realized my mistake I patched it up and voila, there were clusters all along.

## Making Beautiful Clusters

##### The notebook to generate all but the first image in this section is [here](https://drive.google.com/file/d/0B_CaIm7pjWpfZ3hlek9rOWZlZTQ/view?usp=sharing). It’s a bit messy though!

One thing that stood out to me while reading about variational autoencoders specifically is that plotting the encoded points can result in some cool visualizations where you can see how the autoencoder practically clusters each digit together. Here’s two great variational autoencoder resources that have these visualizations and that I found useful while making my own VAE [1](https://jmetzen.github.io/2015-11-27/vae.html), [2](https://blog.keras.io/building-autoencoders-in-keras.html). The following image is from the first post:

![Two dimension mnist, from Jan Hendrik Metzen](/assets/tf_3/from_jmetzen.png)

I thought it was really cool that most digits had a visible cluster. However, it looks like all of the clusters were largely sandwiched together into the shape of a normal distribution. So I decided to try and make those clusters as cluster-y as possible. I also figured I would try to make use of the fact that the mnist dataset has labels. I thought maybe that extra data would help the encoder sort-of classify each digit while encoding it. So I decided to keep track of the center of the cluster for each digit and then instead of tracking the KL divergence for all of the points, I would separately calculate it for each class. So now I had two terms in my loss function, the reconstruction loss (how similar the input and output images look), and the the class-based kl divergence (how close the distribution of the encoded points is to a normal distribution centered around the center of the each digit’s cluster). With this new class-based kl divergence we can even sample the distribution for each class and use the decoder as a generative model for each class! Let’s see how the clusters ended up turning out:

![Two dimension mnist with clusters](/assets/tf_3/latent_space_clusters_still.png)

Those are some pretty nice looking clusters! Much more well defined and separate than when trained with the single kl divergence. Although there are definitely a couple mis-classifications. Just look for colored dots in clusters they don’t belong to. I’m amazed that I was actually able to separate out each digit so well. I wasn’t able to actually accomplish this until the night before posting this! Before the network always struggled to separate out the 4s and the 9s, which is understandable given how similarly the two numbers can be written. I also decoded a grid of points to create a really neat visualization of what digits in different parts of the encoded space look like:

![Two dimension mnist decoded with clusters](/assets/tf_3/latent_space_decoded_still.png)

With that you can really see how the network encodes some different styles of writing each digit, such as how slanted the digit should be drawn. Because I was curious about what was going on during the training of this autoencoder, I decided to take little snapshots of this encoded space throughout the training (I sampled in a sorta logarithmic fashion). And together they make a really cool animated gif where we can see how the network starts from nothing and eventually you see clusters emerge and then become more distinct from each other. It’s awesome to see the training in action, and I feel like it helps me understand how the stochastic gradient descent algorithm (Adam, specifically in this case) works. In the gifs you might notice there are a couple moments where the points seem to move very quickly. Those moments are from when I decrease the training rate, going from 1e-3, to 5e-4, and finally 1e-5. They are also because I sampled training steps in a sorta-logarithmic fashion, meaning I grabbed samples much more frequently early on in training.

![Latent space clusters during training](/assets/tf_3/latent_space_clusters.gif)

And we can see a grid of the latent space decoded while training here. You can match up where the clusters of each digit is above in the gif below to see how the digits look throughout each cluster. Neat stuff! Hopefully the two gifs are synced on this page, they’re pretty big files so be patient if they take a while to load.

![Latent space decoded during training](/assets/tf_3/latent_space_decoded.gif)

There are many other things I tried to make the clusters better defined. For example I tried incorporating more functions into the loss. One such function was a measure of closeness between each cluster center. I made it so that value would become large if the clusters were all on top of each other, and small if the cluster centers had a reasonable amount of distance from each other. Unfortunately my implementation didn’t work as I had hoped, so I removed that part of the loss from my final notebook. Getting this loss function to work took some effort. When I first started approaching this problem I calculated the cluster centers outside of tensorflow every few thousand training steps, then passed them into tensorflow. But by doing that I couldn’t include it in the closeness loss function because it was a constant. My solution was to make it a variable in tensorflow that each step became a little bit closer to the center of that batch’s digits. Doing this involved a lot of fun matrix manipulation, which I probably could have done a bit more elegantly, but it was a lot of fun to work out nonetheless. I tried a couple other things, and I have many more things I want to try, but I don’t want this post ramble on any more than it already is, so I won’t go into the details. Please feel free to message me and I’ll share some of the other things I did.

## Moving Forward

So I now have a much better understanding of how Tensorflow works and what it’s capable, I’ve steadily been learning about the field of deep learning, and I’ve been trying to keep up with the state of the art. I’ve kind of grown bored of the mnist dataset after working with it for the last few months. So I think for my next post/project I’ll try to tackle something completely different. A few areas that I want to look more deeply into are (Deep Q learning)[https://deepmind.com/research/dqn/] (used by Deepmind to play Atari games), recurrent neural networks, and audio processing/generation. Each of those have had some impressive breakthroughs in the last few years and seem like such neat topics to explore. I also want to look more deeply into approaches to unsupervised learning other than autoencoders. I’ve found that using the L2-norm loss function for images tends to just make things blurry. I think I might explore using [generative adversarial networks](https://arxiv.org/abs/1406.2661) or even the [jigsaw approach](https://arxiv.org/pdf/1603.09246.pdf). I’m thinking I might try applying similar logic of the jigsaw method to audio or video, by creating a network that sorts clips of audio or frames of video. I really wish I could just screw around with these ideas all day, but unfortunately I haven’t won the lottery yet :). Thanks for reading!




